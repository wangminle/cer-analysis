# ASR Character Accuracy Comparison Tool

A Python-based tool for batch comparing character accuracy rates between ASR (Automatic Speech Recognition) transcription results and standard text, with multi-tokenizer support.

## âœ¨ Core Features

### ğŸ¯ Multi-Tokenizer Support
- **Jieba Tokenizer**: Default choice, high-speed segmentation, suitable for daily use
- **THULAC Tokenizer**: Developed by Tsinghua University, high-precision segmentation, suitable for professional analysis
- **HanLP Tokenizer**: Deep learning model, highest precision, suitable for research environments

### ğŸš€ Smart Features
- âœ… **Automatic Tokenizer Detection**: Detects installed tokenizers at startup
- âœ… **Smart Fallback Mechanism**: Automatically fallback to jieba when tokenizers are unavailable
- âœ… **Real-time Status Display**: GUI shows tokenizer status and version information
- âœ… **Dependency-free Demo**: Complete architecture demonstration without additional dependencies

### ğŸ“Š Advanced Functions
- Batch import ASR transcription result documents and standard annotation documents
- Drag-and-drop to establish one-to-one correspondence between ASR results and annotation files
- Automatically calculate Character Accuracy Rate
- Count document character information
- Support exporting statistical results in TXT or CSV format
- Support multiple text encodings (UTF-8, GBK, GB2312, GB18030, ANSI)
- **Filler word filtering**: Optional filtering of filler words like "å—¯", "å•Š"
- **Optimized user interface**: Larger result display area, more user-friendly experience
- **Asynchronous computation**: Background thread processing with real-time progress updates
- **Task cancellation**: Cancel long-running calculations at any time
- **CLI tool**: Command-line interface for batch processing and automation
- **Preprocessing pipeline**: Modular and configurable text preprocessing system

## ğŸ“¦ Installation & Dependencies

### Installation
```bash
# Option 1: Using pipenv (recommended)
pipenv install              # Install core dependencies
pipenv install --dev        # Install core + dev dependencies

# Option 2: Using pip + pyproject.toml
pip install -e .            # Install core dependencies (editable mode)
pip install -e .[all]       # Install all optional tokenizers
pip install -e .[dev]       # Install dev/test tools
```

#### Dependency Description
**Core Dependencies (Required):**
- `jieba>=0.42.1`: Default Chinese tokenizer

**Recommended:**
- `python-Levenshtein>=0.12.2`: Efficient edit distance calculation (built-in pure Python fallback)

**Optional Tokenizers:**
- `thulac>=0.2.0`: THULAC high-precision tokenizer (`pip install -e .[thulac]`)
- `hanlp>=2.1.0`: HanLP deep learning tokenizer (`pip install -e .[hanlp]`)

## ğŸ® Usage

### 1. GUI Mode (Recommended)

```bash
python3 dev/src/main_with_tokenizers.py
```

#### Operation Steps:
1. **Select Tokenizer**: Choose the desired tokenizer in the top dropdown
2. **Check Status**: Confirm tokenizer status shows green âœ“, click "Tokenizer Info" for detailed information
3. **Import Files**:
   - Left: Click "Select ASR Files" to batch import ASR transcription results
   - Right: Click "Select Annotation Files" to batch import standard annotation files
4. **Establish Correspondence**: Adjust file order by drag-and-drop
5. **Configure Options**: Check "Filter Filler Words" as needed
6. **Calculate Statistics**: Click "Start Calculation" button
7. **View Results**: Result table shows detailed statistics, including tokenizer type used
8. **Export Data**: Click "Export Results" to save as file

#### Interface Function Description:
- **Tokenizer Selection Area**: Select and manage tokenizers
- **File Selection Area**: Import and manage file lists
- **Control Area**: Statistics button and option configuration
- **Result Display Area**: Detailed statistical result table

### 2. CLI Mode (Command Line Interface)

```bash
# Single file comparison
python3 dev/src/cli.py --asr path/to/asr.txt --ref path/to/ref.txt --tokenizer jieba

# Batch processing
python3 dev/src/cli.py --asr-dir path/to/asr_files/ --ref-dir path/to/ref_files/ --output results.csv

# With filler word filtering
python3 dev/src/cli.py --asr asr.txt --ref ref.txt --filter-fillers --output result.csv

# List available tokenizers
python3 dev/src/cli.py --list-tokenizers
```

#### CLI Features:
- **Single file/Batch processing**: Process one or multiple file pairs
- **Tokenizer selection**: Choose from available tokenizers
- **Filler word filtering**: Optional language filler filtering
- **Export formats**: CSV or TXT output
- **Automation friendly**: Perfect for CI/CD pipelines

### 3. Batch Processing Mode

For GUI-based batch file processing:
```bash
python3 dev/src/main_with_tokenizers.py
```
Then follow the interface operation steps for batch import and processing.

## ğŸ¯ Tokenizer Selection Guide

### Jieba Tokenizer
- **Performance**: âš¡ High Speed
- **Accuracy**: â­â­â­ Medium
- **Use Cases**: Daily batch processing, quick verification
- **Advantages**: Fast speed, low resource usage, good compatibility

### THULAC Tokenizer
- **Performance**: âš¡âš¡ Medium Speed
- **Accuracy**: â­â­â­â­ High Precision
- **Use Cases**: Professional analysis, high quality requirements
- **Advantages**: Developed by Tsinghua University, academic standards, accurate POS tagging

### HanLP Tokenizer
- **Performance**: âš¡ Slower (first use requires model download)
- **Accuracy**: â­â­â­â­â­ Highest Precision
- **Use Cases**: Research environments, highest precision requirements
- **Advantages**: Deep learning models, multi-task support, continuous updates

## ğŸ“ Character Accuracy Calculation Method

Uses the complement of Character Error Rate (CER):

```
Character Accuracy = 1 - CER = 1 - (S + D + I) / N
```

Where:
- **S**: Number of substitution errors
- **D**: Number of deletion errors
- **I**: Number of insertion errors
- **N**: Total number of characters in the standard text

### ğŸ”§ Improved Calculation Process

1. **Tokenization Preprocessing**: Use selected tokenizer for text segmentation
2. **Text Normalization**: Process full/half-width characters, unify numerical expressions
3. **Filler Word Filtering (Optional)**: Filter filler words like "å—¯", "å•Š", "å‘¢"
4. **Character Position Localization**: Precisely locate each character's position in original text
5. **Edit Distance Calculation**: Use Levenshtein distance algorithm
6. **Error Analysis**: Identify substitution, deletion, insertion errors with visualization

## ğŸ“ Project Structure

```
cer-analysis/                            # Project root
â”œâ”€â”€ src/                                 # ğŸ§  Source code
â”‚   â””â”€â”€ cer_tool/                        # Python package
â”‚       â”œâ”€â”€ __init__.py                  # Package metadata + compatibility wrapper
â”‚       â”œâ”€â”€ __main__.py                  # python -m cer_tool entry point
â”‚       â”œâ”€â”€ metrics.py                   # ğŸ“Š CER metrics calculation engine
â”‚       â”œâ”€â”€ cli.py                       # ğŸ’» CLI command-line interface
â”‚       â”œâ”€â”€ gui.py                       # ğŸ¨ GUI interface (tkinter)
â”‚       â”œâ”€â”€ preprocessing.py             # ğŸ”„ Preprocessing pipeline
â”‚       â”œâ”€â”€ file_utils.py                # ğŸ“‚ File utility functions
â”‚       â””â”€â”€ tokenizers/                  # Tokenizer module
â”‚           â”œâ”€â”€ base.py                  # Abstract base class
â”‚           â”œâ”€â”€ factory.py               # Factory (singleton + cache)
â”‚           â”œâ”€â”€ jieba_tokenizer.py       # Jieba implementation
â”‚           â”œâ”€â”€ thulac_tokenizer.py      # THULAC implementation
â”‚           â””â”€â”€ hanlp_tokenizer.py       # HanLP implementation
â”‚
â”œâ”€â”€ tests/                               # ğŸ§ª Tests (127 pytest cases)
â”‚   â”œâ”€â”€ pytest.ini                       # pytest configuration
â”‚   â”œâ”€â”€ test_boundary.py                 # Boundary condition tests (20)
â”‚   â”œâ”€â”€ test_cli.py                      # CLI tests (14)
â”‚   â”œâ”€â”€ test_preprocessing.py            # Preprocessing tests (25)
â”‚   â”œâ”€â”€ test_core_metrics.py             # Core metrics tests (33)
â”‚   â”œâ”€â”€ test_tokenizers_unit.py          # Tokenizer unit tests (23)
â”‚   â”œâ”€â”€ test_with_pytest_marks.py        # Legacy integration tests (12)
â”‚   â””â”€â”€ reports/                         # ğŸ“‹ Test reports
â”‚
â”œâ”€â”€ docs/                                # ğŸ“š Documentation
â”œâ”€â”€ dev/                                 # ğŸ›  Development auxiliary
â”œâ”€â”€ assets/                              # ğŸ¨ Static assets
â”œâ”€â”€ release/                             # ğŸ“¦ Release artifacts
â”œâ”€â”€ ref/                                 # ğŸ“‹ Reference materials (read-only)
â”œâ”€â”€ pyproject.toml                       # Package definition + dependency layers
â”œâ”€â”€ Pipfile                              # pipenv dependency management
â”œâ”€â”€ CLAUDE.md                            # Project knowledge base (AI context)
â””â”€â”€ README.md                            # Project description
```

## ğŸ”§ Troubleshooting

### Common Issues

**Q: How to handle unavailable tokenizers?**
A: Check if corresponding dependencies are installed:
```bash
pip install thulac    # Install THULAC
pip install hanlp     # Install HanLP
```

**Q: Why is HanLP slow on first use?**
A: HanLP needs to download deep learning models, first use requires patience. Recommend using in good network environment.

**Q: How to quickly verify functionality?**
A: Use sample files in the ref/demo directory for testing:
```bash
# Use GUI interface to import sample files from ref/demo directory for testing
python3 dev/src/main_with_tokenizers.py
```

**Q: How to choose the right tokenizer?**
A: Refer to tokenizer selection guide, choose based on speed and accuracy needs:
- For speed: Choose Jieba
- For balance: Choose THULAC
- For precision: Choose HanLP

## ğŸ†• Version Features

### Current Version Highlights
- ğŸ¯ **Multi-tokenizer Architecture**: Support for three mainstream Chinese tokenizers
- ğŸš€ **Smart Switching**: Automatic detection and graceful fallback
- ğŸ¨ **Optimized Interface**: More user-friendly experience with async processing
- ğŸ“Š **Detailed Statistics**: Enhanced result display and analysis
- ğŸ”§ **Drag-and-Drop Sorting**: Intuitive file correspondence management
- âš¡ **Asynchronous GUI**: Non-blocking interface with background computation
- ğŸ¯ **Task Control**: Real-time progress updates and cancellation support
- ğŸ’» **CLI Tool**: Professional command-line interface for automation
- ğŸ”„ **Preprocessing Pipeline**: Flexible and modular text preprocessing
- ğŸ§ª **Layered Testing**: 127 pytest cases with reports layer separation

### Backward Compatibility
- âœ… Maintain original API interfaces unchanged
- âœ… Default to jieba tokenizer
- âœ… Support original file formats and encodings

## ğŸ“ Technical Support

For issues, please check:
- `ref/demo/` directory - Contains sample files for testing
- `docs/` directory - Detailed technical documentation
- `tests/reports/` directory - Test reports and strategy docs
- `pyproject.toml` - Complete dependency configuration

## ğŸ“„ License

This project is released under an open source license, see `LICENSE` file for details.

---

ğŸ‰ **Experience multi-tokenizer switching now to improve ASR character accuracy analysis precision and efficiency!** 